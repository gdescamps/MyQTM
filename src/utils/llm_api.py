import hashlib
import os
import pickle
import time
from threading import Lock

from google import genai

from src.utils.path import get_project_root

# Define the path to the cache file
CACHE_FILE = get_project_root() + "/llm_cache.pkl"

_llm_cache_lock = Lock()
_llm_cache = {}

# Load the existing cache or initialize a new dictionary
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "rb") as f:
        _llm_cache = pickle.load(f)

prices = {
    "gemini-2.5-flash": {
        "input_price_per_million": 0.1,
        "output_price_per_million": 0.4,
    },
    "gemini-2.0-flash": {
        "input_price_per_million": 0.1,
        "output_price_per_million": 0.4,
    },
    "gemini-2.0-flash-lite": {
        "input_price_per_million": 0.075,
        "output_price_per_million": 0.3,
    },
}


def call_llm(prompt, model_name="gpt-4o", cache=True):
    """
    Calls a language model (LLM) API to generate a response based on the given prompt.

    This function uses caching to avoid redundant API calls for the same prompt and model.
    If a cached response exists, it is returned directly. Otherwise, the function interacts
    with the OpenAI API to generate a response, handles potential errors with retries, and
    stores the result in the cache.

    Args:
        prompt (str): The input text or query to send to the language model.
        model (str, optional): The name of the language model to use. Defaults to "gpt-4o".

    Returns:
        str: The response generated by the language model.
        float: The cost of the API call in USD, if applicable (e.g., for Gemini models).

    Raises:
        Exception: If an error occurs during the API call, it retries indefinitely with a delay.

    Notes:
        - The function requires an environment variable `OPENAI_API_KEY` to authenticate with the API.
        - Responses are cached using a SHA-256 hash of the prompt and model name as the key.
        - The cache is persisted to a file defined by the `CACHE_FILE` variable.
    """
    global _llm_cache
    global _llm_cache_lock

    if cache:
        # Generate a unique key based on the prompt and model
        cache_key = hashlib.sha256(f"{model_name}:{prompt}".encode("utf-8")).hexdigest()
        # Check if the response is already in the cache
        with _llm_cache_lock:
            if cache_key in _llm_cache:
                return _llm_cache[cache_key], 0.0

    if "gemini" in model_name:
        client = genai.Client(
            vertexai=True,
            project=os.getenv("PROJECT_ID"),
            location="us-east1",
        )

        while True:
            try:
                response = client.models.generate_content(
                    model=model_name, contents=prompt
                )
                break
            except Exception as e:
                time.sleep(10)

        input_token = response.usage_metadata.prompt_token_count
        output_token = response.usage_metadata.candidates_token_count
        price = input_token * prices[model_name]["input_price_per_million"]
        price += output_token * prices[model_name]["output_price_per_million"]
        price /= 1000000
        response = response.text

    # Save the response in the cache
    if cache:
        with _llm_cache_lock:
            _llm_cache[cache_key] = response

    return response, price


def save_llm_cache():
    global _llm_cache
    global _llm_cache_lock
    with _llm_cache_lock:
        with open(CACHE_FILE, "wb") as f:
            pickle.dump(_llm_cache, f, protocol=pickle.HIGHEST_PROTOCOL)
